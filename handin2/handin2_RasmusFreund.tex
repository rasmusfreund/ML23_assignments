\documentclass[english,11pt,a4paper,titlepage]{report}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{babel}
\title{Handin 2 - Neural Nets for Multiclass Classification}
\author{Rasmus Freund - 201700273}
\begin{document}
	\maketitle
	
	\section*{Part I: Derivative}
	Given a one-hot-label vector $y$ with $y_j = 1$, show that:
	\begin{equation*}
	\frac{\partial L}{\partial z_i} = - \delta_{i,j} + \frac{1}{\sum_{a=1}^{k} e^{z_a}} \times e^{z_i} = -\delta_{i,j} + softmax(z)_i
	\end{equation*}
	where $\delta_{i,j} = 1$ if $i=j$ and zero otherwise.
	
	\subsubsection{Solution}
	Softmax is defined as:
	\begin{equation*}
	softmax(z)_i = \frac{e^{z_i}}{\sum_{a=1}^{k} e^{z_a}}
	\end{equation*}
	Therefore, the negative log-likelihood for the true class $j$ is:
	\begin{equation*}
	L(z) = -ln(softmax(z)_j) = -ln \left( \frac{e^{z_j}}{\sum_{a=1}^{k} e^{z_a}} \right)
	\end{equation*}
	The derivate of $L(z)$ w.r.t. $z_i$ when $i=j$ is then:
	
	\begin{equation*}
	\frac{\partial L}{\partial z_i} = -\frac{1}{softmax(z)_j} \times \frac{\partial softmax(z)_j}{\partial z_i}
	\end{equation*}
	Calculating the partial derivative of $softmax(z)_j$ w.r.t. $z_i$
	\begin{align*}
	\frac{\partial softmax(z)_j}{\partial z_i} &= \frac{\partial}{\partial z_i} \left( \frac{e^{z_i}}{\sum_{a=1}^{k} e^{z_a}} \right) \\[7pt]
											   &= \frac{e^{z_j \times \sum_{a=1}^{k} (e^{z_a})-e^{z_j} \times e^{z_i}}}{(\sum_{a=1}^{k} e^{z_a})^2} \\[7pt]
											   &= \frac{e^{z_j}}{\sum_{a=1}^{k} e^{z_a}} - \left( \frac{e^{z_j}}{\sum_{a=1}^{k} e^{z_a}} \right)^2 \\[7pt]
											   &= softmax(z)_j - (softmax(z)_j)^2
	\end{align*}
	Plugging this back into the original partial derivative
	\begin{align*}
	\frac{\partial L}{\partial z_i} &= -\frac{1}{softmax(z)_j} \times (softmax(z)_j - (softmax(z)_j)^2) \\
									&= -1 + softmax(z)_j
	\end{align*}
	Since $\delta_{i,j}=1$ if $i=j$
	\begin{equation*}
	\underline{\underline{\frac{\partial L}{\partial z_i} = -\delta_{i,j} + softmax(z)_i}}
	\end{equation*}
	
	\section*{Part II: Implementation and test}	
\end{document}